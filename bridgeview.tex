\documentclass{beamer}
\usepackage{geometry}
\usepackage[T1]{fontenc}
%\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{datetime}
\usepackage{booktabs}
\usepackage{listings}

\usepackage{tikz}
\usepackage{keyval}
\usepackage{ifthen}

\usepackage[figwidth=8cm]{todonotes}

\usepackage[british]{babel}

\usetheme{metropolis}

\title{Generate 3D models for large-scale objects}
\author{Jaan Toots \and Viorica Patraucean}
\institute[CSIC]{Cambridge Centre for Smart Infrastructure and
  Construction}
\date{\formatdate{24}{10}{2016}}

\begin{document}

\maketitle

\section{Approach}

\begin{frame}{Deep learning for semantic and geometric segmentation}
  \begin{figure}[h]\centering
    \only<1>{\includegraphics[width=9cm]{ninewells-photo.jpg}}
    \only<2>{\includegraphics[width=9cm]{ninewells-view.png}}
  \end{figure}
  
\end{frame}
\note{The project aims at generating BIM (Building Information Model)
  for existing bridges in an automatic way. This requires modelling
  the geometry and the materials of the bridge components. Material
  recognition from images is (relatively) solved. The challenge is on
  the geometry side.}
\note{\ldots which amounts to recognising for each bridge part what it
  represents and what shape it has, e.g.\ a column that has
  cylindrical shape. Once this info is available, a geometric fitting
  step is needed to get the parameters for each part. Due to the
  recent success of deep learning in various tasks like image
  labelling, object detection, semantic segmentation, we decided to
  use a deep learning based approach as well.}

\begin{frame}{Neural networks}
  \begin{figure}[h]\centering
    \include{neural-net}
  \end{figure}
\end{frame}
\note{Deep learning relies on deep neural networks, which are powerful
  models, with high capacity to learn from examples. But large
  training sets are required because the models can have millions of
  parameters. Moreover, training a neural network requires high
  computational resources. Existing methods deal mainly with text,
  images, or videos, and existing computational resources (GPUs) can
  accommodate them.}
\note{For our problem, we would prefer images and videos, but methods
  for estimating depth and 3D from images are not sufficiently
  accurate. So we need to use point clouds, which provide accurate
  geometry. But point clouds contain millions of 3D points, having
  huge memory requirements, and cannot be used efficiently for GPU
  processing.}

\subsection{Input}

\begin{frame}{Unordered point-cloud}
  \begin{figure}[h]\centering
    \includegraphics[width=9cm]{ninewells-cloud.png}
  \end{figure}
\end{frame}
\note{This is because we need to deal with unordered point clouds,
  like the ones the laser-scanners provide after registration. An
  unordered point cloud is a set of points with 3D coordinates. This
  is opposed to ordered point-clouds, e.g.\ occupancy grids or depth
  maps.}

\begin{frame}{Occupancy grid}
  \begin{figure}[h]\centering
    \include{grid}
  \end{figure}
\end{frame}
\note{An occupancy grid is a 3D grid where each cell is marked
  ``occupied'' if there is a point inside or ``free'' otherwise. Most
  of existing deep learning approaches for 3D point-clouds use this
  representation with 3D (volumetric) neural networks. But they work
  only with small objects (chairs, tables). The requirements for large
  structures like bridges would be prohibitive. Moreover, the memory
  would be wasted since in our case most of the cells would be marked
  as ``free''.}

\begin{frame}{Depth map}
  \begin{figure}[h]\centering
    \includegraphics[width=8cm]{depth.png}
  \end{figure}
\end{frame}
\note{So we chose to represent a point cloud through RGBD images,
  without any loss of information. A depth map is an image where each
  pixel encodes the distance between the camera and the 3D points;
  here colour-coded: cold colours represent shorter distances.}

\subsection{Segmentation of bridges}

\begin{frame}{Segmentation of bridges}
  Main steps:
  \begin{itemize}
  \item Render RGB and depth images
  \item Segmentation of images
  \item Back-project labels into point-cloud
  \end{itemize}
  Because real data is difficult to obtain, we use synthetically
  textured bridge models.
\end{frame}

\section{Dataset}

\begin{frame}{Overview}
  \begin{itemize}
  \item 30 models of bridges
  \item 4535 rendered RGBD images
    \begin{itemize}
    \item 4100 training
    \item 435 test
    \end{itemize}
  \item Hierarchical labels
    \begin{itemize}
    \item Geometric labels (12)
    \item Class labels (7)
    \item Part labels (28)
    \end{itemize}
  \end{itemize}
\end{frame}

\subsection{Models}

\begin{frame}{Models}
  Start with models from various sources and prepare for
  rendering. Add landscape, roads and vegetation.

  \begin{figure}[h]\centering
    \include{aw}
  \end{figure}
\end{frame}

\begin{frame}[plain]
  \includegraphics[width=\textwidth]{blender.png}
\end{frame}

\subsection{Bridgeview}

\begin{frame}{Bridgeview}
  Scripts to render visual, semantic and depth images using
  Blender.
  
  \lstinputlisting[language=sh]{sample.sh}
\end{frame}

\begin{frame}{Bridgeview}
  \include{graph}
\end{frame}

\begin{frame}{TODO}
  Figures!
\end{frame}

\section{Model}

\begin{frame}{Part-based segmentation}
  \begin{description}
  \item[Baseline] U-NET trained on part labels
  \item[Proposed architecture] Hierarchical U-NET --- each decoder
    module outputs a different label (class label, part label,
    geometric label)
  \end{description}
  \begin{figure}[h]\centering
    \vspace{-1cm}
    \def\svgwidth{10cm}
    \include{net}
    \vspace{-0.5cm}
  \end{figure}
\end{frame}
\note{We propose a dedicated architecture for part-based segmentation,
  where the network predicts different labels at each level. The
  network first has to recognise if the pixel belongs to the bridge
  class or other class (e.g.\ vegetation, terrain), and then recognise
  the part and the geometric shape.  Experiments show that this
  approach is better than just predicting directly only the part. In
  the hierarchical case we inject more supervision, hence results are
  better.}

\section{Results}

\begin{frame}{Results}
  \begin{block}{Experiments}
    \begin{enumerate}
    \item RGB vs RGB-D using the baseline architecture
    \item Flat vs hierarchical using RGB-D input
    \end{enumerate}
  \end{block}
  \begin{block}{Evaluation}
    \begin{description}
    \item[Global avg.\ accuracy] among all the pixels, how many have
      been labelled correctly on average per image
    \item[Per class avg.\ accuracy] how many pixels have been labelled
      correctly for each class on average
    \end{description}
  \end{block}
\end{frame}
\note{Two sets of experiments:\@ RGB vs.\ RGB-D: to see if adding
  depth helps or if appearance is enough for classification; flat vs
  hierarchical: to prove that the different levels of supervision are
  improving the performance. For quantitative evaluation, we use two
  measures.}

\begin{frame}{Quantitative results}
  \begin{tabular}{lll}
    \toprule
    Architecture/input & Global accuracy & Per class accuracy \\
    \midrule
    RGB baseline & $93.4072$ & $20.64704$ \\
    RGB+D baseline & $95.0932$ & $22.63335$ \\
    RGB+D hierarchical & $96.59791$ & $28.53001$ \\
    \bottomrule
  \end{tabular}
\end{frame}
\note{The quantitative results show that adding depth helps over RGB
  alone. This is because depth tends to have less variety, it is more
  reliable compared to texture, and also RGB can be more influenced by
  shadows, noise.}
\note{The hierarchical training improves significantly the results.
  It can be noted that although the global accuracy is very high, the
  per class accuracy is very low. This is because there are a few very
  small and rare classes (like bearings) which have (as expected) a
  very low accuracy, bringing the overall average accuracy to a low
  level.}

\begin{frame}{Qualitative results: hierarchical U-NET with RGB+D as
    input}
  \begin{figure}[h]\centering
    \only<1>{
      \includegraphics[width=10cm]{ex1.png}
      \vspace{1em}
      \includegraphics[width=10cm]{ex2.png}
    }
    \only<2>{
      \includegraphics[width=10cm]{ex3.png}
      \vspace{1em}
      \includegraphics[width=10cm]{ex4.png}
    }
    \only<3>{
      \includegraphics[width=10cm]{ex5.png}
      \vspace{1em}
      \includegraphics[width=10cm]{ex6.png}
    }
  \end{figure}
\end{frame}
\note{Qualitative results on test images (images that were not used
  during training):\@ The accuracy is very good at the class level,
  terrain, vegetation, bridge are very well recognised. The main
  parts of the bridge are also well identified: columns, piers,
  abutment, deck.}
\note{There are of course mistakes as well, like in the picture on
  top, the network unifies the two piers.  But overall it does get the
  classes, even the smaller ones like handrails.}
\note{\ldots and light-poles. There is a lot of potential in this
  approach. The challenge now is to get some real labelled data, to
  fine-tune the network on real data, so that it is capable of
  segmenting any real image of a bridge.}

\section{Conclusion}

\begin{frame}{Summary}
  Data generation scripts are open source (GPL) and available from

  \begin{center}
    \href{https://github.com/jaantoots/bridgeview}{github.com/jaantoots/bridgeview}
  \end{center}

  Model files and the complete dataset will be made available.
\end{frame}

\begin{frame}[standout]
  Questions?
\end{frame}

\end{document}
